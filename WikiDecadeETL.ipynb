{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subjective-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "import wikipediaapi as wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sixth-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractETL(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def transform():\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "contrary-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiDecadeETL(AbstractETL):\n",
    "    \"\"\"\n",
    "    Getting data specifically from WikiPedia.\n",
    "    In this case wiki pages on historical events data by decade\n",
    "    \"\"\"\n",
    "    ignore_sections = {'Pronunciation varieties','Name for the decade',\n",
    "                       'Further reading','References','External links', \n",
    "                       \"Notes\", \"Footnotes\", \"Wikisource reference work\"}\n",
    "    \n",
    "    root_query = \"List of decades, centuries, and millennia\"\n",
    "    \n",
    "    def __init__(self, query:str=root_query)->None:\n",
    "        self.query = query\n",
    "        self._service = wk.Wikipedia(\"en\", extract_format=wk.ExtractFormat.WIKI)\n",
    "        self.page = self._service.page(query)\n",
    "        self.core_sections = {};\n",
    "        self.coreSect_sub = {}\n",
    "        self.core_df = None\n",
    "        \n",
    "    def get_drange_links(self, start:int, stop:int) -> dict:\n",
    "        \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "        This function filters the decade range of interest and is specific to \n",
    "        the root_query\n",
    "        \"\"\"\n",
    "        \n",
    "        drange = [f\"{str(i)}s\" if str(i)[-2:] != \"00\" else f\"{str(i)}s (decade)\" for i in range(start, stop+1, 10)]\n",
    "        \n",
    "        drange_links = dict(zip(drange, map(self.page.links.get, drange)))\n",
    "        \n",
    "        return drange_links\n",
    "        \n",
    "    \n",
    "    def get_page_sections(self)->Tuple[list, dict]:\n",
    "        \"\"\"\n",
    "        Returns dict of wiki page sections, subsections and text\n",
    "        \"\"\"\n",
    "        main_sections = self.page.sections # We have to get the section before getting the section mapping\n",
    "        all_sections_dict = self.page._section_mapping # section mapping is empty if above is not executed first\n",
    "\n",
    "        return main_sections, all_sections_dict\n",
    "\n",
    "    \n",
    "    def core_section_extractor(self)->None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Params: decade\n",
    "        Returns dict of sections_title of key interest that will later be used to extract a sections content\n",
    "            main_section_title : list of subsections\n",
    "        \"\"\"\n",
    "        \n",
    "        main_sections, all_sections_dict = self.get_page_sections()\n",
    "        \n",
    "        # All sections. Main, Subsections and Sections to ignore\n",
    "        all_section_titles = list(all_sections_dict.keys())\n",
    "\n",
    "        # Only the core sections including \"See also\"\n",
    "        core_section_titles = [s.title for s in main_sections if s.title not in self.ignore_sections]\n",
    "        \n",
    "\n",
    "        # Storing the core section indices according to their position in the all_section_title list\n",
    "        core_indices = {k: all_section_titles.index(k) for k in  core_section_titles}\n",
    "\n",
    "        # Convinience variable \n",
    "        indices_lst = list(core_indices.keys())\n",
    "\n",
    "\n",
    "        # Store the core title with a list of its subsections\n",
    "        # core_indices = {\"People\": 14, \"See Also\": 16} - # People is on index 14 on all_section_index with possible 2 subsections\n",
    "        # index_lst = [\"People\", \"See Also\"] - People is at index 0 of core_indices.keys()\n",
    "        # core_dict = {\"People\":[\"World Leaders\", \"Business Leaders\"]}\n",
    "        self.coreSect_sub = {indices_lst[i]:all_section_titles[core_indices[indices_lst[i]]+1: core_indices[indices_lst[i+1]]] \n",
    "                                   for i in range(len(indices_lst)-1)}\n",
    "        \n",
    "        sect_titles = self.coreSect_sub.keys()\n",
    "        # Subseting the all_sections_dict to only the core_sections with subs embeded\n",
    "        self.core_sections = dict(zip(sect_titles, map(all_sections_dict.get, sect_titles)))\n",
    "        \n",
    "        \n",
    "    def get_subtitle(self, val):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the subsections title list for a section in an entry.\n",
    "        \n",
    "        If subsections do not exist return the section title\n",
    "        \"\"\"\n",
    "        \n",
    "        res = val\n",
    "        if self.coreSect_sub[val]:\n",
    "            res = self.coreSect_sub[val]\n",
    "        return res\n",
    "    \n",
    "    def get_subtext(self, val):\n",
    "        \"\"\"\n",
    "        Returns the a subsections full texts for an entry\n",
    "        \"\"\"\n",
    "        return self.page.section_by_title(val).full_text()\n",
    "    \n",
    "    def get_df(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creating a dataframe from extracted data\n",
    "        \"\"\"\n",
    "        \n",
    "        core_sections = self.core_sections\n",
    "        \n",
    "        temp_df = pd.DataFrame.from_dict(core_sections, orient=\"index\", \n",
    "                                         columns=[\"text\"]).reset_index().rename(columns={\"index\": \"section\"})\n",
    "        \n",
    "        temp_df[\"sub_section\"] = temp_df[\"section\"].apply(self.get_subtitle)\n",
    "        \n",
    "        temp_df = temp_df.explode(\"sub_section\", ignore_index=True)\n",
    "        \n",
    "        temp_df[\"text\"] = temp_df[\"sub_section\"].apply(self.get_subtext)\n",
    "        \n",
    "        temp_df[\"decade\"] = self.query\n",
    "        \n",
    "        self.core_df = temp_df\n",
    "        \n",
    "    def extract(self):\n",
    "        # Mainly for fetching the data we want from Wikipedia\n",
    "        self.core_section_extractor()\n",
    "    \n",
    "    def transform(self):\n",
    "        # Processing the raw data retaining only the parts we want\n",
    "        self.extract()\n",
    "        \n",
    "        self.get_df()\n",
    "    \n",
    "    def load(self):\n",
    "        # Loading the semi-processed data in data frame format.\n",
    "        self.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "looking-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(query:str)->WikiDecadeETL:\n",
    "    \n",
    "    decade = WikiDecadeETL(query)\n",
    "    decade.load()\n",
    "    return decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "entertaining-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinedSects_df(start:int=1900, stop:int=2020)->dict:\n",
    "    \n",
    "    lofdcm = WikiDecadeETL()\n",
    "    drange_links = lofdcm.get_drange_links(start, stop)\n",
    "    \n",
    "    combined_dict = {query: run(query).core_df for query in drange_links.keys()}\n",
    "    \n",
    "    combined_df = pd.concat(combined_dict.values(), ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "complex-virgin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(start:int=1900, stop:int=2020, version=2, _func=None, annot=''):\n",
    "\n",
    "    combined_df = _func(start, stop)\n",
    "\n",
    "    combined_df.to_csv(f\"data/v{version}_{start}_{stop}s_{annot}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-professor",
   "metadata": {},
   "source": [
    "We run this cell only when we want to save a combined df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "parallel-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "alpine-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinedSumm_df(start:int=1900, stop:int=2020)->dict:\n",
    "    \n",
    "    lofdcm = WikiDecadeETL()\n",
    "    drange_links = lofdcm.get_drange_links(start, stop)\n",
    "    \n",
    "    combined_dict = {query: run(query).page.summary for query in drange_links.keys()}\n",
    "    \n",
    "    combined_df = pd.DataFrame.from_dict(tst, orient=\"index\", columns=[\"summary\"])\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"index\": \"decade\"})\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-landscape",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "- [x] Finalize of the regex filter for the extracting the correct decade range\n",
    "    - Used mapping instead\n",
    "- [x] Iterate through the decades and concatenate all the resulting dataframes\n",
    "- [ ] Create a datafra\n",
    "- [ ] Optionally drop rows without years in the text \n",
    "    * Only retaining text with year in order to extract years of note in the decade\n",
    "    * For mvp we can proceed as is then implement this on phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-round",
   "metadata": {},
   "source": [
    "### Summary DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "stunning-writing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combinedSumm_df(start:int=1900, stop:int=2020)->dict:\n",
    "    \n",
    "    lofdcm = WikiDecadeETL()\n",
    "    drange_links = lofdcm.get_drange_links(start, stop)\n",
    "    \n",
    "    combined_dict = {query: run(query).page.summary for query in drange_links.keys()}\n",
    "    \n",
    "    combined_df = pd.DataFrame.from_dict(tst, orient=\"index\", columns=[\"summary\"])\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"index\": \"decade\"})\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "confused-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df(version=0, _func=get_combinedSumm_df, annot=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-caution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8 (DS)",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
